# ğŸ“¢ Silent Tokens, Loud Effects  

> **Understanding the Hidden Impact of Padding in LLMs**  

[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)  

Padding tokens are commonly used in large language models (LLMs) to equalize sequence lengths during batched inference.  
When mishandled, these tokens can **leak into computation**, but the extent of their influence has remained unclear.  

In this work, we systematically investigate padding effects across three open-source model families:  
- ğŸ¦™ **Llama**  
- ğŸ’ **Gemma**  
- ğŸ¦ **Qwen**  

We evaluate controlled amounts of pad tokens along **four axes**:  

| Axis               | Impact Observed |
|--------------------|-----------------|
| ğŸ”¬ Internal Activations | Drift in hidden representations |
| âœï¸ Generation Quality | Reduction in output fluency & coherence |
| âš–ï¸ Bias            | Altered demographic biases |
| ğŸ›¡ï¸ Safety          | Weakened safeguards & harmful outputs |

â¡ï¸ **Key takeaway**: Even small amounts of padding can harm LLM behavior. Padding requires **careful handling** in deployment pipelines.  

---

## ğŸš€ Getting Started  

### 1. Clone the Repository  
```bash
git clone https://github.com/wr0om/silent_tokens_loud_effects.git
cd silent_tokens_loud_effects
```

### 2. Set Up the Environment  
```bash
pip install -r requirements.txt
```

### 3. Run the Experiments  

#### Activation, Generation & Safety  
```bash
python full_activation_analysis.py
python full_generation_analysis.py
python full_safety_analysis.py
```

#### Bias (per demographic category)  
```bash
python full_bias_analysis.py --model_path meta-llama/Llama-3.1-8B-Instruct --category_num 0
python full_bias_analysis.py --model_path meta-llama/Llama-3.1-8B-Instruct --category_num 1
...
python full_bias_analysis.py --model_path meta-llama/Llama-3.1-8B-Instruct --category_num 10
```

> ğŸ’¡ Replace `meta-llama/Llama-3.1-8B-Instruct` with any other model used in the paper.  

---

## ğŸ“Š Results & Visualization  

- All results are saved under the `results/` directory.  
- Bias experiment outputs are saved separately in `BBQ/results/`.  

Use the included Jupyter notebooks for visualization:  
- `full_activation_analysis.ipynb`  
- `full_generation_analysis.ipynb`  
- `full_safety_analysis.ipynb`  
- `BBQ/analysis_scripts/bias_analysis.ipynb`  

---

## ğŸ™ Acknowledgements  
We adapt the following GitHub repositories in our experiments:  
- https://github.com/thu-coai/AISafetyLab  
- https://github.com/neulab/BARTScore  
- https://github.com/nyu-mll/BBQ  
- https://github.com/andyrdt/refusal_direction

## ğŸ“œ Citation  
If you use this work, please cite our paper (coming soon).  
