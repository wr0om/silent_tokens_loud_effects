"""
PARDEN Defense Method
============================================
This Class achieves a defense method describe in the paper below.

Paper title: PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition
Arxiv link: https://arxiv.org/pdf/2405.07932
Source repository: https://github.com/Ed-Zh/PARDEN
"""
from aisafetylab.defense.inference_defense.base_defender import PostprocessDefender
from aisafetylab.defense.inference_defense.defender_texts import PARDEN_DEFAULT_TEXT, PARDEN_DEFAULT_IN_CONTEXT_EXAMPLE
from aisafetylab.models.base_model import Model
from aisafetylab.models.local_model import LocalModel
from nltk.translate.bleu_score import sentence_bleu
import re
from loguru import logger
 

class PardenDefender(PostprocessDefender):
    def __init__(self, model: Model, tokenizer, model_name, max_new_tokens=512, threshold=0.6, parden_prompt=PARDEN_DEFAULT_TEXT,in_context_example = PARDEN_DEFAULT_IN_CONTEXT_EXAMPLE):
        """
        Initialize the PardenDefender.
        
        Args:
            model (Model): The model to generate repetition response (here refers to the generation model)
            max_new_tokens (int): Maximum number of tokens to generate for repetition response
            threshold (float): Similarity threshold for repetition detection (0~1)
            parden_prompt (str): Template for parden prompt
            in_context_example (str): In-context example prompt
        """
        self.model = LocalModel(model, tokenizer,model_name)
        self.model_name = model_name
        self.max_new_tokens = max_new_tokens
        self.threshold = threshold
        self.parden_prompt = parden_prompt
        self.in_context_example = in_context_example
    
    def apply_prompt_template(self, output):
        """
        Apply the PARDEN prompt template and system prompt according to the model used to generate repetition response
        
        Args:
            output (str): The output generated by the primary model which needs to be examined
        
        Returns:
            str: PARDEN prompt template with system prompt
        """
        prompt = self.parden_prompt.replace("{IN_CONTEXT_EXAMPLE}", self.in_context_example).replace("{answer}", output)
        if "vicuna" in self.model_name:
            vicuna_prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nUSER: {prompt}\n""".replace("{END_CONTROL_TOKEN}","ASSISTANT: ") 
            return vicuna_prompt 
        elif "llama-2" in self.model_name:
            llama2_prompt = f"""<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n\n{prompt}""".replace("{END_CONTROL_TOKEN}","[/INST]")
            return llama2_prompt
        elif "llama-3" in self.model_name:
            llama3_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}""".replace("{END_CONTROL_TOKEN}","<|eot_id|><|start_header_id|>assistant<|end_header_id|>")
            return llama3_prompt
        elif "mistral" in self.model_name:
            mistral_prompt = f"""<s>[INST] Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity. {prompt}""".replace("{END_CONTROL_TOKEN}","[/INST]")
            return mistral_prompt
        else:
            raise ValueError(f"Unsupported model name: {self.model_name}")


    def calculate_bleu_score(self, output, repetition_response, clip = True):
        """
        Calculate the BLEU score between the original output and the repetition response
        
        Args:
            output (str): The output generated by the primary model which needs to be examined
            repetition_response (str): Response to the repetition prompt
            clip (bool): Whether to clip the output to the same length as the repetition response

        Returns:
            float: BLEU score between the two responses (0~1)
        """
        if clip:
            output = output.replace('/n', '')
            repetition_response = repetition_response.replace('/n', '')
            num_words = min(len(output.split(" ")), len(repetition_response.split(" ")))

            output = output.strip().split(" ")[:num_words]
            repetition_response = repetition_response.strip().split(" ")[:num_words]

            output = " ".join(output)
            repetition_response = " ".join(repetition_response)
            
            output = re.sub(r"\s+", " ", output)
            repetition_response = re.sub(r"\s+", " ", repetition_response)
        else:
            output = re.sub(r"\s+", " ", output)
            repetition_response = re.sub(r"\s+", " ", repetition_response)

        return sentence_bleu([output], repetition_response)



    def defend(self, output, query=None):
        """
        Apply the PARDEN defense mechanism to the provided output

        Args:
            output (str): The output generated by the primary model which needs to be examined

        Returns:
            str: The defended output which is considered safe if manipulation is detected, potentially altered or annotated
        """
        logger.debug(f"judging: \n{output}")
        repetition_messages = self.apply_prompt_template(output)

        repetition_response = self.model.chat(repetition_messages,use_chat_template=False, max_new_tokens=self.max_new_tokens)
        logger.debug(f"repetition result: \n{repetition_response}")

        b_score = self.calculate_bleu_score(output, repetition_response, clip = True)
        logger.debug(f"b_score: {b_score}")

        if b_score > self.threshold:
            return output
        else:
            return repetition_response