"""
Self-Exam Defense Method
============================================
This Class achieves a defense method describe in the paper below.

Paper title: LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked
Arxiv link: https://arxiv.org/pdf/2308.07308
Source repository: https://github.com/poloclub/llm-self-defense
"""

from aisafetylab.defense.inference_defense.base_defender import PostprocessDefender
from aisafetylab.models.base_model import Model
from aisafetylab.models.local_model import LocalModel
from aisafetylab.defense.inference_defense.defender_texts import SELF_EXAM_DEFAULT_TEXT, SORRY_RESPONSE
from loguru import logger

class SelfExamDefender(PostprocessDefender):
    """
    Defender class that implements a self-examination defense strategy for language models, This strategy involves a model judging its own output to detect potential tricks or harmful manipulations.
    """
    def __init__(self, model: Model, tokenizer, model_name, judge_prompt=SELF_EXAM_DEFAULT_TEXT):
        """
        Initialize the SelfExamDefender Defender. 

        Args:
            model (Model): The judge model (here especially means the generation model).
            judge_prompt (str): The judge prompt.
        """
        self.model = LocalModel(model, tokenizer, model_name)
        self.judge_prompt = judge_prompt
        print(self.judge_prompt)
        
    def defend(self, output, query=None):
        """
        Applies the self-examination defense mechanism to the provided output.

        Args:
            output (str): The output generated by the primary model which needs to be examined.

        Returns:
            str: The defended output which is considered safe if manipulation is detected, potentially altered or annotated.
        """
        logger.debug(f"judging: \n{output}")
        judging_messages = [
            {
                "role": "user",
                "content": self.judge_prompt.replace("content", f"{output}")
            }
        ]
        
        judge = self.model.chat(judging_messages)
        
        logger.debug(f"judging result: \n{judge}")
        if "yes" in judge.lower():
            return SORRY_RESPONSE
        else:
            return output
        