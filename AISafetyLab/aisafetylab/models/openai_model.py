from openai import OpenAI
from fastchat.conversation import get_conv_template
from time import sleep
from loguru import logger
from .base_model import Model
import openai
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
from tqdm import tqdm

class OpenAIModel(Model):
    def __init__(self, model_name, base_url, api_key, generation_config=None):
        self.model_name = model_name
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.conversation = get_conv_template('chatgpt')
        self.generation_config = generation_config if generation_config is not None else {}

        self.API_RETRY_SLEEP = 10
        self.API_ERROR_OUTPUT = "$ERROR$"
        self.API_QUERY_SLEEP = 0.5
        self.API_MAX_RETRY = 5
        self.API_TIMEOUT = 20
        self.API_LOGPROBS = True
        self.API_TOP_LOGPROBS = 20

    def set_system_message(self, system_message: str):
        """
        Sets a system message for the conversation.
        :param str system_message: The system message to set.
        """
        self.conversation.system_message = system_message
    
    def generate(self, messages, clear_old_history=True, max_try=30, try_gap=5, gap_increase=5, **kwargs):
        """
        Generates a response based on messages that include conversation history.
        :param list[dict]|list[str]|str messages: A list of messages or a single message string.
                                       User and assistant messages should alternate.
        :param bool clear_old_history: If True, clears the old conversation history before adding new messages.
        :return str: The response generated by the OpenAI model based on the conversation history.
        """
        
        ##print("messages: ", messages)
        # not consistent with parallel request
        # if clear_old_history:
        #     self.conversation.messages = []
        # if isinstance(messages, str):
        #     messages = [messages]
        # if isinstance(messages[0], dict):
        #     if 'role' in messages[0] and messages[0]['role'] == 'system':
        #         self.conversation.set_system_message(messages[0]['content'])
        #         messages = messages[1:]
        #     messages = [msg['content'] for msg in messages]
        # for index, message in enumerate(messages):
        #     self.conversation.append_message(self.conversation.roles[index % 2], message)
            
        if isinstance(messages, str):
            processed_messages = [{'role': 'user', 'content': messages}]
        else:
            processed_messages = messages
            
        cur = 0
        temp_gen_config = self.generation_config.copy()
        if kwargs:
            temp_gen_config.update(kwargs)
        while cur < max_try:
            try:
                # logger.debug(f"model_name: {self.model_name}, messages: {self.conversation.to_openai_api_messages()}")

                response = self.client.chat.completions.create(
                    model=self.model_name,
                    # messages=self.conversation.to_openai_api_messages(),
                    messages = processed_messages,
                    **temp_gen_config
                )
                # logger.debug(f"response: {response}")
                content = response.choices[0].message.content
                if content is None:
                    raise Exception("Empty response")
                return content
            except Exception as e:
                logger.error(f"Failed to generate response within {self.model_name}: {e}, retrying...")
                cur += 1
                if cur < max_try:
                    sleep(try_gap)
                    try_gap += gap_increase
                else:
                    raise e
    
    def chat(self, messages, clear_old_history=True, max_try=30, try_gap=5, **kwargs):
        return self.generate(messages, clear_old_history, max_try, try_gap, **kwargs)
    
    def batch_chat(self, batch_messages, clear_old_history=True, max_try=30, try_gap=5, max_workers=10, show_progress=True, **kwargs):
        """
        并发处理多个聊天请求
        
        :param list batch_messages: 批量消息列表
        :param bool clear_old_history: 是否清除历史
        :param int max_try: 最大尝试次数
        :param int try_gap: 重试间隔时间
        :param int max_workers: 最大并发线程数
        :param bool show_progress: 是否显示进度条
        :param kwargs: 其他参数传递给chat方法
        :return: 按输入顺序排列的响应列表
        """
        if not batch_messages:
            return []
            
        # 为了保证返回结果的顺序与输入顺序一致，我们需要跟踪每个请求的索引
        results = [None] * len(batch_messages)
        total = len(batch_messages)
        
        def process_single_request(index_and_message):
            index, messages = index_and_message
            try:
                response = self.chat(messages, clear_old_history, max_try, try_gap, **kwargs)
                return index, response
            except Exception as e:
                logger.error(f"Error processing request {index}: {e}")
                return index, f"ERROR: {str(e)}"
        
        # 限制并发线程数，防止资源过度使用
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # 提交所有任务，并记录它们的索引
            future_to_index = {executor.submit(process_single_request, (i, msg)): i 
                              for i, msg in enumerate(batch_messages)}
            
            # 处理完成的任务，并显示进度条
            if show_progress:
                # 使用tqdm创建进度条
                pbar = tqdm(total=total, desc="Processing requests", unit="req")
                completed = 0
                
                for future in as_completed(future_to_index):
                    index, response = future.result()
                    logger.debug(f'index: {index}\nprompt: {batch_messages[index]}\nresponse: {response}')
                    results[index] = response
                    completed += 1
                    pbar.update(1)
                    pbar.set_postfix({"completed": f"{completed}/{total}"})
                
                pbar.close()
            else:
                # 不显示进度条
                for future in as_completed(future_to_index):
                    index, response = future.result()
                    results[index] = response
        
        return results
    
    def get_response(self, prompts_list, max_n_tokens=None, no_template=False, gen_config={}):
        if isinstance(prompts_list[0], str):
            prompts_list = [[{'role': 'user', 'content': prompt}] for prompt in prompts_list]
        
        convs = prompts_list
        print(convs)
        outputs = []
        for conv in convs:
            output = self.API_ERROR_OUTPUT
            for _ in range(self.API_MAX_RETRY):
                try:
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=conv,
                        max_tokens=max_n_tokens,
                        **gen_config,
                        timeout=self.API_TIMEOUT,
                        logprobs=self.API_LOGPROBS,
                        top_logprobs=self.API_TOP_LOGPROBS,
                        seed=0,
                    )
                    response_logprobs = [
                        dict((response.choices[0].logprobs.content[i_token].top_logprobs[i_top_logprob].token, 
                                response.choices[0].logprobs.content[i_token].top_logprobs[i_top_logprob].logprob) 
                                for i_top_logprob in range(self.API_TOP_LOGPROBS)
                        )
                        for i_token in range(len(response.choices[0].logprobs.content))
                    ]
                    output = {'text': response.choices[0].message.content,
                            'logprobs': response_logprobs,
                            'n_input_tokens': response.usage.prompt_tokens,
                            'n_output_tokens': response.usage.completion_tokens,
                    }
                    break
                except openai.OpenAIError as e:
                    print(type(e), e)
                    time.sleep(self.API_RETRY_SLEEP)

                time.sleep(self.API_QUERY_SLEEP)
            outputs.append(output)
        return outputs
        